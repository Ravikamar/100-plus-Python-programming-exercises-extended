{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuwLHCKd86yBFLHKOsZVcK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikamar/100-plus-Python-programming-exercises-extended/blob/master/SQL_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsldiEVvSrEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "try:\n",
        "  from pyspark.sql import SparkSession\n",
        "except:\n",
        "  os.system(\"pip install pyspark\")\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "v7ojIV1TGiqO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Medium Level:\n",
        "2️⃣\n",
        "3️⃣\n",
        "4️⃣ Write a query to calculate the running total of sales by date.\n",
        "5️⃣ Find employees who earn more than the average salary in their department.\n",
        "6️⃣ Write a query to find the most frequently occurring value in a column.\n",
        "7️⃣ Fetch records where the date is within the last 7 days from today.\n",
        "8️⃣ Write a query to count how many employees share the same salary.\n",
        "9️⃣ How do you fetch the top 3 records for each group in a table?\n",
        "🔟 Retrieve products that were never sold (hint: use LEFT JOIN).\n",
        "\n",
        "💡 Challenging Level:\n",
        "1️⃣ Retrieve customers who made their first purchase in the last 6 months.\n",
        "2️⃣ How do you pivot a table to convert rows into columns?\n",
        "3️⃣ Write a query to calculate the percentage change in sales month-over-month.\n",
        "4️⃣ Find the median salary of employees in a table.\n",
        "5️⃣ Fetch all users who logged in consecutively for 3 days or more.\n",
        "6️⃣ Write a query to delete duplicate rows while keeping one occurrence.\n",
        "7️⃣ Create a query to calculate the ratio of sales between two categories.\n",
        "8️⃣ How would you implement a recursive query to generate a hierarchical structure?\n",
        "9️⃣ Write a query to find gaps in sequential numbering within a table.\n",
        "🔟 Split a comma-separated string into individual rows using SQL.\n",
        "\n",
        "💡 Advanced Problem-Solving:\n",
        "1️⃣ Rank products by sales in descending order for each region.\n",
        "2️⃣ Fetch all employees whose salaries fall within the top 10% of their department.\n",
        "3️⃣ Identify orders placed during business hours (e.g., 9 AM to 6 PM).\n",
        "4️⃣ Write a query to get the count of users active on both weekdays and weekends.\n",
        "5️⃣ Retrieve customers who made purchases across at least three different categories.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "c_HKeKwFAc68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1.M Write a query to find the second highest salary in an employee table.\")\n",
        "\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'Alice', 50000),\n",
        "(2, 'Bob', 60000),\n",
        "(3, 'Charlie', 70000),\n",
        "(4, 'David', 60000)\n",
        "AS t(id, name, salary)\"\"\").createOrReplaceTempView(\"tab\")\n",
        "\n",
        "spark.sql(\"select salary from (select salary, row_number() over(order by salary desc) rn from tab) t where rn=2 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmcwzGXh_ARf",
        "outputId": "0a0827bf-eab9-4d97-bbe2-c8834526982b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.M Write a query to find the second highest salary in an employee table.\n",
            "+------+\n",
            "|salary|\n",
            "+------+\n",
            "| 60000|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2.M Fetch all employees whose names contain the letter “a” exactly twice.\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM VALUES\n",
        "(1, 'Ananya'),\n",
        "(2, 'Aarav'),\n",
        "(3, 'Aakash'),\n",
        "(4, 'Aman'),\n",
        "(5, 'Bala')\n",
        "AS t(id, name);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select * from tab where (length(lower(name)) - length(replace(lower(name),'a',''))) = 2 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAj4N2Y2_ccr",
        "outputId": "5c8164f8-d398-46bf-c16c-3c23e1b7e1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.M Fetch all employees whose names contain the letter “a” exactly twice.\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  4|Aman|\n",
            "|  5|Bala|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"3.M How do you retrieve only duplicate records from a table?\")\n",
        "spark.sql('''SELECT * FROM VALUES\n",
        "(1, 'Red', 100),\n",
        "(2, 'Blue', 200),\n",
        "(3, 'Red', 100),\n",
        "(4, 'Green', 300)\n",
        "AS t(id, color, value);\n",
        "''').createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select * from tab where id in (select count(1) from tab group by color,value having count(1)>1)\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYvPvLR8_jMW",
        "outputId": "d038292f-ae03-4ee9-9e63-1c9f8b9af445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.M How do you retrieve only duplicate records from a table?\n",
            "+---+-----+-----+\n",
            "| id|color|value|\n",
            "+---+-----+-----+\n",
            "|  2| Blue|  200|\n",
            "+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4.M Write a query to calculate the running total of sales by date.\")\n",
        "spark.sql('''SELECT * FROM VALUES\n",
        "('2024-06-01', 100),\n",
        "('2024-06-02', 200),\n",
        "('2024-06-03', 150),\n",
        "('2024-06-04', 250),\n",
        "('2024-06-05', 100),\n",
        "('2024-06-06', 300)\n",
        "AS t(sale_date, amount);''').createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select sale_date, amount, sum(amount) over(order by sale_date rows between 2 preceding and current row) running_total from tab\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKin7E_g_0Ip",
        "outputId": "4ae90e7f-bbb5-4df7-ad26-46ae5e039429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.M Write a query to calculate the running total of sales by date.\n",
            "+----------+------+-------------+\n",
            "| sale_date|amount|running_total|\n",
            "+----------+------+-------------+\n",
            "|2024-06-01|   100|          100|\n",
            "|2024-06-02|   200|          300|\n",
            "|2024-06-03|   150|          450|\n",
            "|2024-06-04|   250|          600|\n",
            "|2024-06-05|   100|          500|\n",
            "|2024-06-06|   300|          650|\n",
            "+----------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5.M Find employees who earn more than the average salary in their department.\")\n",
        "spark.sql('''SELECT * FROM VALUES\n",
        "(1, 'Alice', 50000, 1),\n",
        "(2, 'Bob', 60000, 1),\n",
        "(3, 'Charlie', 70000, 2),\n",
        "(4, 'David', 40000, 2)\n",
        "AS t(id, name, salary, department_id)''').createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"\"\"\n",
        "select e.* from tab e\n",
        "join\n",
        "(select department_id, avg(salary) avg_salary from tab group by department_id) d\n",
        "on e.department_id = d.department_id\n",
        "where e.salary > d.avg_salary\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQIwbZBZ_0FN",
        "outputId": "587fb155-fdac-4c99-f52d-9de47c9b67d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.M Find employees who earn more than the average salary in their department.\n",
            "+---+-------+------+-------------+\n",
            "| id|   name|salary|department_id|\n",
            "+---+-------+------+-------------+\n",
            "|  2|    Bob| 60000|            1|\n",
            "|  3|Charlie| 70000|            2|\n",
            "+---+-------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"6.M Write a query to find the most frequently occurring value in a column.\")\n",
        "spark.sql('''\n",
        "SELECT * FROM VALUES\n",
        "('Apple'),\n",
        "('Banana'),\n",
        "('Apple'),\n",
        "('Orange'),\n",
        "('Apple')\n",
        "AS t(fruit);''').createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select fruit, count(1) cnt from tab group by fruit order by cnt desc limit 1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJZPDYMg_0Ca",
        "outputId": "07b71cc8-7505-414e-b0b0-32b68365bbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.M Write a query to find the most frequently occurring value in a column.\n",
            "+-----+---+\n",
            "|fruit|cnt|\n",
            "+-----+---+\n",
            "|Apple|  3|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"7.M Fetch records where the date is within the last 7 days from today.\")\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "('2025-03-27'),\n",
        "('2025-05-27'),\n",
        "('2025-06-01'),\n",
        "('2025-06-03')\n",
        "AS t(date_column);\"\"\").createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select * from tab where date_column >= current_date - interval 7 day\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCefLqzY_z_0",
        "outputId": "0bf0c097-06b6-4236-e5fd-2b2251a2adde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.M Fetch records where the date is within the last 7 days from today.\n",
            "+-----------+\n",
            "|date_column|\n",
            "+-----------+\n",
            "| 2025-05-27|\n",
            "| 2025-06-01|\n",
            "| 2025-06-03|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ How many share the same salary\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'Alice', 50000),\n",
        "(2, 'Bob', 60000),\n",
        "(3, 'Charlie', 50000),\n",
        "(4, 'David', 70000)\n",
        "AS t(id, name, salary)\"\"\").createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select salary, count(1) from tab group by salary having count(1)>1\").show()"
      ],
      "metadata": {
        "id": "__7IqrWS_z9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5928cf48-672e-47fb-9e4d-03d98631b658"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|salary|count(1)|\n",
            "+------+--------+\n",
            "| 50000|       2|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9️⃣ How do you fetch the top 3 records for each group in a table?\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'A', 90),\n",
        "(2, 'A', 80),\n",
        "(3, 'A', 70),\n",
        "(4, 'A', 60),\n",
        "(5, 'B', 88),\n",
        "(6, 'B', 77)\n",
        "AS t(id, group_column, score);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select * from (select *, row_number() over(partition by group_column order by score desc) rn from tab) t where rn<=3\").show()"
      ],
      "metadata": {
        "id": "qBS4YeGG_z6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5833c3a6-77eb-4b46-a44c-00ec456d0c41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+-----+---+\n",
            "| id|group_column|score| rn|\n",
            "+---+------------+-----+---+\n",
            "|  1|           A|   90|  1|\n",
            "|  2|           A|   80|  2|\n",
            "|  3|           A|   70|  3|\n",
            "|  5|           B|   88|  1|\n",
            "|  6|           B|   77|  2|\n",
            "+---+------------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#🔟 Products Never Sold (LEFT JOIN)\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM VALUES\n",
        "(1, 'Laptop'),\n",
        "(2, 'Mouse'),\n",
        "(3, 'Keyboard')\n",
        "AS products(product_id, name)\"\"\").createOrReplaceTempView(\"products\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM VALUES\n",
        "(1, 1000),\n",
        "(2, 500)\n",
        "AS sales(product_id, amount)\"\"\").createOrReplaceTempView(\"sales\")\n",
        "\n",
        "spark.sql(\"select * from products left join sales on products.product_id = sales.product_id where sales.product_id is null\").show()"
      ],
      "metadata": {
        "id": "25hj2NrE_z3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c64d37-7a14-400a-8d25-d8a17d5f9051"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+------+\n",
            "|product_id|    name|product_id|amount|\n",
            "+----------+--------+----------+------+\n",
            "|         3|Keyboard|      NULL|  NULL|\n",
            "+----------+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Customers with First Purchase in Last 6 Months\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, '2024-12-01'),\n",
        "(2, '2025-02-15'),\n",
        "(3, '2025-04-01'),\n",
        "(4, '2023-11-20')\n",
        "AS t(customer_id, purchase_date);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")\n"
      ],
      "metadata": {
        "id": "r9khFl0u_z1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Pivot Table (Gender per Department\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'M', 10),\n",
        "(2, 'F', 10),\n",
        "(3, 'M', 20),\n",
        "(4, 'F', 20),\n",
        "(5, 'M', 10)\n",
        "AS t(emp_id, gender, department_id);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "z1h8UDVu_zyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Month-over-Month Sales\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "('2025-01', 1000),\n",
        "('2025-02', 1200),\n",
        "('2025-03', 1000),\n",
        "('2025-04', 1500)\n",
        "AS t(sale_month, amount);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "Q3pi1rr4_zqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Median Salary\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 40000),\n",
        "(2, 50000),\n",
        "(3, 60000),\n",
        "(4, 70000),\n",
        "(5, 80000)\n",
        "AS t(emp_id, salary);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "uVl7zqJbeGci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Users with 3+ Consecutive Logins\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, '2025-06-01'),\n",
        "(1, '2025-06-02'),\n",
        "(1, '2025-06-03'),\n",
        "(2, '2025-06-01'),\n",
        "(2, '2025-06-03')\n",
        "AS t(user_id, login_date);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "_cSXfiL1eHkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6️⃣ Delete Duplicate Rows, Keep One\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'John', 50000),\n",
        "(2, 'John', 50000),\n",
        "(3, 'Alice', 60000)\n",
        "AS t(id, name, salary);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "HKQYpzupeIpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7️⃣ Sales Ratio Between Categories\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'A', 100),\n",
        "(2, 'B', 50),\n",
        "(3, 'A', 200),\n",
        "(4, 'B', 150)\n",
        "AS t(id, category, amount);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "ITFkO7yDePh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'CEO', NULL),\n",
        "(2, 'Manager1', 1),\n",
        "(3, 'Manager2', 1),\n",
        "(4, 'Dev1', 2),\n",
        "(5, 'Dev2', 2)\n",
        "AS t(id, name, manager_id);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "fSS9n3OzeM2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9️⃣ Gaps in Sequential Numbers\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1), (2), (4), (5), (7)\n",
        "AS t(current_number);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "rP5XcIh0hJON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔟 Split Comma-Separated String into Rows\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'apple,banana,grape'),\n",
        "(2, 'orange,mango')\n",
        "AS t(id, names);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "OuXhgs91hJLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Rank Products by Sales in Region\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "('North', 'P1', 100),\n",
        "('North', 'P2', 200),\n",
        "('South', 'P1', 300),\n",
        "('South', 'P3', 150)\n",
        "AS t(region, product_id, sale_amount);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "ZWjSveaYhJIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Top 10% Salaries per Department\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'A', 50000),\n",
        "(2, 'A', 55000),\n",
        "(3, 'A', 60000),\n",
        "(4, 'B', 70000),\n",
        "(5, 'B', 80000)\n",
        "AS t(id, department_id, salary);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "Vp8hf2AphJFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Orders During Business Hours (9 AM - 6 PM)\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, '2025-06-03 08:59:00'),\n",
        "(2, '2025-06-03 09:30:00'),\n",
        "(3, '2025-06-03 17:59:00'),\n",
        "(4, '2025-06-03 18:01:00')\n",
        "AS t(order_id, order_time);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "TeP-GWuHhJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Users Active on Weekdays and Weekends\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, '2025-06-01'),  -- Sunday\n",
        "(1, '2025-06-03'),  -- Tuesday\n",
        "(2, '2025-06-02'),  -- Monday\n",
        "(3, '2025-06-01'),  -- Sunday\n",
        "(3, '2025-06-02')   -- Monday\n",
        "AS t(user_id, activity_date);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "0A8AVacEhI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Customers Purchasing Across 3+ Categories\n",
        "spark.sql(\"\"\"SELECT * FROM VALUES\n",
        "(1, 'Electronics'),\n",
        "(1, 'Clothing'),\n",
        "(1, 'Home'),\n",
        "(2, 'Clothing'),\n",
        "(2, 'Clothing')\n",
        "AS t(customer_id, category_id);\n",
        "\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "YhMAriAchI8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "xjYd2ge1hI4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "-gsZaBx_hI0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "C5trVSK4hIuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "VZDuiciBhIsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "2UZu0hRBhIpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "hw3_ITpmhImP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Recursive Hierarchy\n",
        "spark.sql(\"\"\"\"\"\").createOrReplaceTempView(\"tab\")"
      ],
      "metadata": {
        "id": "XI2NgGGkhIjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-onh5UThIRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"create table mytab as select * from values(1,'a'),(1,'a'),(1,'b'),(1,'b'),(1,'c'),(1,'d'),(1,'d') t(a,b)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtHOZwKq3W0p",
        "outputId": "ebaef398-ab73-4f5a-ca1e-a7f427b61dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from mytab\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Mmx4B45-Ck",
        "outputId": "b626d454-1777-462b-bf67-d166fbcd797e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|  1|  b|\n",
            "|  1|  c|\n",
            "|  1|  d|\n",
            "|  1|  d|\n",
            "|  1|  a|\n",
            "|  1|  a|\n",
            "|  1|  b|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "| id|   name|    city|start_date|end_date|is_current|\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "|101|  Alice|New York|2023-01-01|    NULL|         Y|\n",
        "|102|    Bob|  London|2023-01-05|    NULL|         Y|\n",
        "|103|Charlie|   Paris|2023-01-10|    NULL|         Y|\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "\n",
        "+---+-----+-----------+\n",
        "| id| name|       city|\n",
        "+---+-----+-----------+\n",
        "|101|Alice|Los Angeles|\n",
        "|102|  Bob|     London|\n",
        "|104|David|     Berlin|\n",
        "+---+-----+-----------+\n",
        "\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "| id|   name|    city|start_date|end_date|is_current|\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "|101|  Alice|New York|2023-01-01|2025-02-09|       N|\n",
        "|101|  Alice|New York|2025-02-09|    NULL|         Y|\n",
        "|102|    Bob|  London|2023-01-05|    NULL|         Y|\n",
        "|103|Charlie|   Paris|2023-01-10|    NULL|         Y|\n",
        "|104|David  |  Berlin|2025-02-09|    NULL|         Y|\n",
        "+---+-------+--------+----------+--------+----------+\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "6Qd5i69Y4aUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9TnEK_V5TPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41AQVlb25SZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for SCD Type 2\n",
        "print('# --> Initial Target table: Historic table with customer information i.e target_df')\n",
        "target_df = spark.sql('''\n",
        "select * from values\n",
        " ('101','Alice','New York','2023-01-01',null, 'Y')\n",
        ",('102','Bob','London','2023-01-05',null, 'Y')\n",
        ",('103','Charlie','Paris','2023-01-10',null, 'Y')\n",
        "t(id, name, city, start_date, end_date, is_current)\n",
        "''')\n",
        "target_df.show()\n",
        "print('#--> incoming Souce Table : Latest snapshot from the source')\n",
        "source_df = spark.sql('''\n",
        "select * from values\n",
        "('101','Alice','Los Angeles'),\n",
        "('102','Bob','London'),\n",
        "('104','David','Berlin')\n",
        "t(id, name, city)\n",
        "''')\n",
        "source_df.show()\n",
        "\n",
        "print('Step 1 --> Identify the changed records')\n",
        "changed_df = target_df.alias(\"tgt\").join(source_df.alias(\"src\"),\"id\",\"left\").filter(expr(\"tgt.name<>src.name or tgt.city <> src.city\")).select(\"id\")\n",
        "\n",
        "\n",
        "print('# # Step 2 --> Mark old record ineactive and update end date')\n",
        "target_df_delta = target_df.alias(\"tgt\").join(changed_df.alias(\"chg\"),\"id\",\"left\")\\\n",
        ".withColumn(\"end_date\",  expr(\"case when chg.id is not null then current_date() else tgt.end_date end\"))\\\n",
        ".withColumn(\"is_current\",expr(\"case when chg.id is not null then 'N' else tgt.is_current end\"))\n",
        "target_df_delta.show()\n",
        "\n",
        "print('# Insert new and updated rewcords')\n",
        "new_and_updated_df = source_df.alias(\"src\").join(target_df.alias(\"trt\"),\"id\",\"left\").select(\"src.*\")\\\n",
        ".withColumn(\"start_date\",current_date())\\\n",
        ".withColumn(\"end_date\",lit(None))\\\n",
        ".withColumn(\"is_active\",lit('Y'))\\\n",
        ".union(target_df_delta)\n",
        "new_and_updated_df.orderBy(\"id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDZu4WxZSr9x",
        "outputId": "7a6f2324-efcc-4ade-f9dc-7d0e92785115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# --> Initial Target table: Historic table with customer information i.e target_df\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "| id|   name|    city|start_date|end_date|is_current|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "|101|  Alice|New York|2023-01-01|    NULL|         Y|\n",
            "|102|    Bob|  London|2023-01-05|    NULL|         Y|\n",
            "|103|Charlie|   Paris|2023-01-10|    NULL|         Y|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "\n",
            "#--> incoming Souce Table : Latest snapshot from the source\n",
            "+---+-----+-----------+\n",
            "| id| name|       city|\n",
            "+---+-----+-----------+\n",
            "|101|Alice|Los Angeles|\n",
            "|102|  Bob|     London|\n",
            "|104|David|     Berlin|\n",
            "+---+-----+-----------+\n",
            "\n",
            "Step 1 --> Identify the changed records\n",
            "# # Step 2 --> Mark old record ineactive and update end date\n",
            "+---+-------+--------+----------+----------+----------+\n",
            "| id|   name|    city|start_date|  end_date|is_current|\n",
            "+---+-------+--------+----------+----------+----------+\n",
            "|101|  Alice|New York|2023-01-01|2025-02-10|         N|\n",
            "|102|    Bob|  London|2023-01-05|      NULL|         Y|\n",
            "|103|Charlie|   Paris|2023-01-10|      NULL|         Y|\n",
            "+---+-------+--------+----------+----------+----------+\n",
            "\n",
            "# Insert new and updated rewcords\n",
            "+---+-------+-----------+----------+----------+---------+\n",
            "| id|   name|       city|start_date|  end_date|is_active|\n",
            "+---+-------+-----------+----------+----------+---------+\n",
            "|101|  Alice|Los Angeles|2025-02-10|      NULL|        Y|\n",
            "|101|  Alice|   New York|2023-01-01|2025-02-10|        N|\n",
            "|102|    Bob|     London|2023-01-05|      NULL|        Y|\n",
            "|102|    Bob|     London|2025-02-10|      NULL|        Y|\n",
            "|103|Charlie|      Paris|2023-01-10|      NULL|        Y|\n",
            "|104|  David|     Berlin|2025-02-10|      NULL|        Y|\n",
            "+---+-------+-----------+----------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_df_delta = target_df.alias(\"tgt\").join(changed_df.alias(\"chg\"),\"id\",\"left\")\\\n",
        ".withColumn(\"start_date\",expr(\"case when chg.id is not null then current_date() else tgt.start_date end\"))\\\n",
        ".withColumn(\"end_date\",expr(\"case when chg.id is not null then Null else tgt.end_date end\"))\\\n",
        ".withColumn(\"is_current\",expr(\"case when chg.id is not null then 'N' else tgt.is_current end\"))\n",
        "target_df_delta.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZkIhxdAIgVL",
        "outputId": "ee79de91-f2ee-4690-8fd8-413c080bfa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+----------+--------+----------+\n",
            "| id|   name|    city|start_date|end_date|is_current|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "|101|  Alice|New York|2025-02-10|    NULL|         N|\n",
            "|102|    Bob|  London|2023-01-05|    NULL|         Y|\n",
            "|103|Charlie|   Paris|2023-01-10|    NULL|         Y|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for SCD Type 2\n",
        "print('Initial Target table: Historic table with customer information i.e target_df')\n",
        "target_df = spark.sql('''\n",
        "select * from values\n",
        " ('101','Alice','New York','2023-01-01',null, 'Y')\n",
        ",('102','Bob','London','2023-01-05',null, 'Y')\n",
        ",('103','Charlie','Paris','2023-01-10',null, 'Y')\n",
        "t(id, name, city, start_date, end_date, is_current)\n",
        "''')\n",
        "target_df.show()\n",
        "print('Incoming Souce Table : Latest snapshot from the source')\n",
        "source_df = spark.sql('''\n",
        "select * from values\n",
        "('101','Alice','Los Angeles'),\n",
        "('102','Bob','London'),\n",
        "('104','David','Berlin')\n",
        "t(id, name, city)\n",
        "''')\n",
        "source_df.show()\n",
        "\n",
        "# identify and udate existing rows\n",
        "updated_df = target_df.alias(\"trg\").join(source_df.alias(\"src\"),\"id\",\"left\")\\\n",
        ".filter(\"trg.name<>src.name or trg.city <> src.city\").select(\"src.*\")\\\n",
        ".withColumn(\"\")\n",
        "updated_df.show()\n",
        "#\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "wcZvwnxCrinD",
        "outputId": "15b86264-9506-4cc8-ddc9-b774abdd16b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Target table: Historic table with customer information i.e target_df\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "| id|   name|    city|start_date|end_date|is_current|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "|101|  Alice|New York|2023-01-01|    NULL|         Y|\n",
            "|102|    Bob|  London|2023-01-05|    NULL|         Y|\n",
            "|103|Charlie|   Paris|2023-01-10|    NULL|         Y|\n",
            "+---+-------+--------+----------+--------+----------+\n",
            "\n",
            "Incoming Souce Table : Latest snapshot from the source\n",
            "+---+-----+-----------+\n",
            "| id| name|       city|\n",
            "+---+-----+-----------+\n",
            "|101|Alice|Los Angeles|\n",
            "|102|  Bob|     London|\n",
            "|104|David|     Berlin|\n",
            "+---+-----+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DataFrame.withColumn() missing 1 required positional argument: 'col'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1c16d196a1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mupdated_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trg.name<>src.name or trg.city <> src.city\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DataFrame.withColumn() missing 1 required positional argument: 'col'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame creation (replace with your actual data loading)\n",
        "data = [(\"1001\", \"Data Structures\", 70),\n",
        "        (\"1001\", \"Java\", 65),\n",
        "        (\"1001\", \"Software Engineering\", 80),\n",
        "        (\"1001\", \"Maths\", 76),\n",
        "        (\"1002\", \"Data Structures\", 80),\n",
        "        (\"1002\", \"Java\", 75),\n",
        "        (\"1002\", \"Software Engineering\", 90),\n",
        "        (\"1002\", \"Maths\", 86)]\n",
        "\n",
        "columns = [\"RollNo\", \"Subject\", \"Mark\"]\n",
        "student_df = spark.createDataFrame(data, columns)\n",
        "student_df.createOrReplaceTempView(\"student\")\n",
        "#student_df.groupBy(\"RollNo\").pivot(\"Subject\", [\"Data Structures\", \"Java\", \"Software Engineering\", \"Maths\"]).sum(\"Mark\").show()\n",
        "\n",
        "spark.sql(\"\"\"select *,Java +maths as s from (\n",
        "  select rollNo, subject, mark from student\n",
        ")\n",
        "pivot(\n",
        "  sum(mark) for subject in ('Data Structures', 'Java', 'Software Engineering', 'Maths')\n",
        ")\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIe4rXebR1ff",
        "outputId": "110f2d8d-eaf9-4200-a43a-7785d11b0773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+----+--------------------+-----+---+\n",
            "|rollNo|Data Structures|Java|Software Engineering|Maths|  s|\n",
            "+------+---------------+----+--------------------+-----+---+\n",
            "|  1002|             80|  75|                  90|   86|161|\n",
            "|  1001|             70|  65|                  80|   76|141|\n",
            "+------+---------------+----+--------------------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write an SQL query to find the employees who are top 3 highest salary earners in each of the departments.\n",
        "# Employee table:\n",
        "# +----+-------+--------+--------------+\n",
        "# | id | name  | salary | departmentId |\n",
        "# +----+-------+--------+--------------+\n",
        "# | 1  | Joe   | 85000  | 1            |\n",
        "# | 2  | Henry | 80000  | 2            |\n",
        "# | 3  | Sam   | 60000  | 2            |\n",
        "# | 4  | Max   | 90000  | 1            |\n",
        "# | 5  | Janet | 69000  | 1            |\n",
        "# | 6  | Randy | 85000  | 1            |\n",
        "# | 7  | Will  | 70000  | 1\n",
        "\n",
        "# Department table:\n",
        "# +----+-------+\n",
        "# | id | name  |\n",
        "# +----+-------+\n",
        "# | 1  | IT    |\n",
        "# | 2  | Sales |\n",
        "# +----+-------+\n",
        "# Output\n",
        "# ---------\n",
        "# empid, empname, empsalary, deptname\n",
        "'''\n",
        "SELECT id,name,salary,departmentName\n",
        "FROM\n",
        "    (SELECT e.id,e.name,e.salary,e.departmentId,d.name AS departmentName,\n",
        "        DENSE_RANK() OVER (PARTITION BY e.departmentId ORDER BY e.salary DESC) as rank_within_department\n",
        "    FROM Employee e JOIN Department d ON e.departmentId = d.id\n",
        "     ) t\n",
        "WHERE rank_within_department <= 3\n",
        "ORDER BY departmentName, rank_within_department;\n",
        "'''\n",
        "''"
      ],
      "metadata": {
        "id": "--vXhpAFfOZl",
        "outputId": "24070984-9109-4e69-8133-55e486b03e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 prompt: write sql for below table to provide given output Sales table\n",
        "# Store,custid,amt\n",
        "# Input\n",
        "spark.sql('''create or replace temp view tab as select * from values\n",
        "(1,101,500),\n",
        "(1,101,300),\n",
        "(1,102,300),\n",
        "(2,101,1000),\n",
        "(2,102,900) as T(Store,custid,amt)''')\n",
        "spark.sql(\"select * from tab\").show()\n",
        "# # Output: find sum of amt spend by 101 in each store\n",
        "# # 1,101,800\n",
        "# # 2,101,1000\n",
        "spark.sql(\"\"\"\n",
        "select store, custid, sum(amt) from tab group by store, custid\n",
        "\"\"\").filter(\"custid='101'\").show()\n"
      ],
      "metadata": {
        "id": "xWkP8KYdqNh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad47f2cf-4ecd-4c7b-892b-1250b28ca68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----+\n",
            "|Store|custid| amt|\n",
            "+-----+------+----+\n",
            "|    1|   101| 500|\n",
            "|    1|   101| 300|\n",
            "|    1|   102| 300|\n",
            "|    2|   101|1000|\n",
            "|    2|   102| 900|\n",
            "+-----+------+----+\n",
            "\n",
            "+-----+------+--------+\n",
            "|store|custid|sum(amt)|\n",
            "+-----+------+--------+\n",
            "|    1|   101|     800|\n",
            "|    2|   101|    1000|\n",
            "+-----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. You will be given 2 tables with some rows and join condition(left,inner,right). Qus. How many rows will be in the output for the respective join condition.\n",
        "\n",
        "Python\n",
        "1. Typical data structure in python( list, dict ) and operatoin on them? You will be asked to write code snipped for example slice the list. list comprehension etc."
      ],
      "metadata": {
        "id": "v-5lgr4rF4A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 SQL There are sales table, meterial table. when material is sold an entry is created in sales table. find the list of  material which have never been sold."
      ],
      "metadata": {
        "id": "LgxUI9bW0OH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What are the windows function?\n",
        "'''\n",
        "Aggregate Window Functions : SUM(), MAX(), MIN(), AVG(). COUNT()\n",
        "Ranking Window Functions : RANK(), DENSE_RANK(), ROW_NUMBER(), NTILE()\n",
        "Value Window Functions : LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()\n",
        "'''\n",
        "''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZFSZBEG00USL",
        "outputId": "71db86f5-bb3a-442a-8cb1-3dd3acd509b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(200100, 1000.00, 600.00, '2008-01-08', 'C00013', 'A003'),\n",
        "(200112, 2000.00, 400.00, '2008-30-05', 'C00016', 'A007'),\n",
        "(200113, 4000.00, 600.00, '2008-10-06', 'C00022', 'A002'),\n",
        "(200122, 2500.00, 400.00, '2008-16-09', 'C00003', 'A004'),\n",
        "(200118, 500.00, 100.00, '2008-20-07', 'C00023', 'A006'),\n",
        "(200121, 1500.00, 600.00, '2008-23-09', 'C00008', 'A004'),\n",
        "(200108, 4000.00, 600.00, '2008-15-02', 'C00008', 'A004'),\n",
        "(200104, 1500.00, 500.00, '2008-13-03', 'C00006', 'A004'),\n",
        "(200106, 2500.00, 700.00, '2008-20-04', 'C00005', 'A002'),\n",
        "(200117, 800.00, 200.00, '2008-20-10', 'C00014', 'A001'),\n",
        "(200123, 500.00, 100.00, '2008-16-09', 'C00022', 'A002'),\n",
        "(200120, 500.00, 100.00, '2008-20-07', 'C00009', 'A002'),\n",
        "(200124, 500.00, 100.00, '2008-20-06', 'C00017', 'A007'),\n",
        "(200126, 500.00, 100.00, '2008-24-06', 'C00022', 'A002'),\n",
        "(200129, 2500.00, 500.00, '2008-20-07', 'C00024', 'A006'),\n",
        "(200127, 2500.00, 400.00, '2008-20-07', 'C00015', 'A003'),\n",
        "(200128, 3500.00, 1500.00, '2008-20-07', 'C00009', 'A002'),\n",
        "(200133, 1200.00, 400.00, '2008-29-06', 'C00009', 'A002')]\n",
        "\n",
        "schema=\"ORD_NUM int,ORD_AMOUNT double,ADVANCE_AMOUNT double,ORD_DATE string,CUST_CODE string,AGENT_CODE string\"\n",
        "\n",
        "df = spark.createDataFrame(data,schema).withColumn(\"ORD_DATE\",F.expr(\"to_date(ord_date,'yyyy-mm-dd')\")).cache()\n",
        "df.createOrReplaceTempView(\"orders\")\n",
        "spark.sql(\"select * from orders order by ORD_DATE, AGENT_CODE\").show(50)"
      ],
      "metadata": {
        "id": "Pe0FsoJpkNCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de45deb5-1bf4-4c5d-af12-560c1e1446a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------------+----------+---------+----------+\n",
            "|ORD_NUM|ORD_AMOUNT|ADVANCE_AMOUNT|  ORD_DATE|CUST_CODE|AGENT_CODE|\n",
            "+-------+----------+--------------+----------+---------+----------+\n",
            "| 200108|    4000.0|         600.0|2008-01-02|   C00008|      A004|\n",
            "| 200104|    1500.0|         500.0|2008-01-03|   C00006|      A004|\n",
            "| 200106|    2500.0|         700.0|2008-01-04|   C00005|      A002|\n",
            "| 200112|    2000.0|         400.0|2008-01-05|   C00016|      A007|\n",
            "| 200126|     500.0|         100.0|2008-01-06|   C00022|      A002|\n",
            "| 200113|    4000.0|         600.0|2008-01-06|   C00022|      A002|\n",
            "| 200133|    1200.0|         400.0|2008-01-06|   C00009|      A002|\n",
            "| 200124|     500.0|         100.0|2008-01-06|   C00017|      A007|\n",
            "| 200120|     500.0|         100.0|2008-01-07|   C00009|      A002|\n",
            "| 200128|    3500.0|        1500.0|2008-01-07|   C00009|      A002|\n",
            "| 200127|    2500.0|         400.0|2008-01-07|   C00015|      A003|\n",
            "| 200118|     500.0|         100.0|2008-01-07|   C00023|      A006|\n",
            "| 200129|    2500.0|         500.0|2008-01-07|   C00024|      A006|\n",
            "| 200100|    1000.0|         600.0|2008-01-08|   C00013|      A003|\n",
            "| 200123|     500.0|         100.0|2008-01-09|   C00022|      A002|\n",
            "| 200122|    2500.0|         400.0|2008-01-09|   C00003|      A004|\n",
            "| 200121|    1500.0|         600.0|2008-01-09|   C00008|      A004|\n",
            "| 200117|     800.0|         200.0|2008-01-10|   C00014|      A001|\n",
            "+-------+----------+--------------+----------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qn 4.1. calculate the running total revenue for each agent in the third quarter:                 --and average revenue\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "select AGENT_CODE, ORD_AMOUNT, ORD_DATE,\n",
        "  avg(ORD_AMOUNT) over(partition by AGENT_CODE order by ORD_DATE) tot_rev\n",
        "from orders\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU2OGxXRlwEk",
        "outputId": "c159fd97-6d81-4b99-c96b-6c2b0c0fc9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+\n",
            "|AGENT_CODE|ORD_AMOUNT|  ORD_DATE|           tot_rev|\n",
            "+----------+----------+----------+------------------+\n",
            "|      A001|     800.0|2008-01-10|             800.0|\n",
            "|      A002|    2500.0|2008-01-04|            2500.0|\n",
            "|      A002|    4000.0|2008-01-06|            2050.0|\n",
            "|      A002|     500.0|2008-01-06|            2050.0|\n",
            "|      A002|    1200.0|2008-01-06|            2050.0|\n",
            "|      A002|     500.0|2008-01-07|2033.3333333333333|\n",
            "|      A002|    3500.0|2008-01-07|2033.3333333333333|\n",
            "|      A002|     500.0|2008-01-09|1814.2857142857142|\n",
            "|      A003|    2500.0|2008-01-07|            2500.0|\n",
            "|      A003|    1000.0|2008-01-08|            1750.0|\n",
            "|      A004|    4000.0|2008-01-02|            4000.0|\n",
            "|      A004|    1500.0|2008-01-03|            2750.0|\n",
            "|      A004|    2500.0|2008-01-09|            2375.0|\n",
            "|      A004|    1500.0|2008-01-09|            2375.0|\n",
            "|      A006|     500.0|2008-01-07|            1500.0|\n",
            "|      A006|    2500.0|2008-01-07|            1500.0|\n",
            "|      A007|    2000.0|2008-01-05|            2000.0|\n",
            "|      A007|     500.0|2008-01-06|            1250.0|\n",
            "+----------+----------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qn 4.2. How many days after the first purchase of a customer was the next purchase made?\n",
        "spark.sql(\"\"\"\n",
        "select cust_code,ord_date, date_diff(ord_date , first_value(ord_date) over(partition by cust_code order by ord_date)) days from orders\n",
        "\"\"\").orderBy(\"cust_code\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihazinCHtYOW",
        "outputId": "5287f3c6-e043-47c8-b44e-78d625f206a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----+\n",
            "|cust_code|  ord_date|days|\n",
            "+---------+----------+----+\n",
            "|   C00003|2008-01-09|   0|\n",
            "|   C00005|2008-01-04|   0|\n",
            "|   C00006|2008-01-03|   0|\n",
            "|   C00008|2008-01-02|   0|\n",
            "|   C00008|2008-01-09|   7|\n",
            "|   C00009|2008-01-06|   0|\n",
            "|   C00009|2008-01-07|   1|\n",
            "|   C00009|2008-01-07|   1|\n",
            "|   C00013|2008-01-08|   0|\n",
            "|   C00014|2008-01-10|   0|\n",
            "|   C00015|2008-01-07|   0|\n",
            "|   C00016|2008-01-05|   0|\n",
            "|   C00017|2008-01-06|   0|\n",
            "|   C00022|2008-01-06|   0|\n",
            "|   C00022|2008-01-06|   0|\n",
            "|   C00022|2008-01-09|   3|\n",
            "|   C00023|2008-01-07|   0|\n",
            "|   C00024|2008-01-07|   0|\n",
            "+---------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qn 3. what is the last highest amount for which an order was sold by an agent?\n",
        "spark.sql(\"\"\"\n",
        "select agent_code, ORD_AMOUNT, ROW_NUMBER() OVER (PARTITION BY agent_code ORDER BY ord_amount DESC, ord_date DESC) as rn  from orders\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kf2FE70wTVj",
        "outputId": "f5244c40-c62f-490c-f09b-88dc3b4ef655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+---+\n",
            "|agent_code|ORD_AMOUNT| rn|\n",
            "+----------+----------+---+\n",
            "|      A001|     800.0|  1|\n",
            "|      A002|    4000.0|  1|\n",
            "|      A002|    3500.0|  2|\n",
            "|      A002|    2500.0|  3|\n",
            "|      A002|    1200.0|  4|\n",
            "|      A002|     500.0|  5|\n",
            "|      A002|     500.0|  6|\n",
            "|      A002|     500.0|  7|\n",
            "|      A003|    2500.0|  1|\n",
            "|      A003|    1000.0|  2|\n",
            "|      A004|    4000.0|  1|\n",
            "|      A004|    2500.0|  2|\n",
            "|      A004|    1500.0|  3|\n",
            "|      A004|    1500.0|  4|\n",
            "|      A006|    2500.0|  1|\n",
            "|      A006|     500.0|  2|\n",
            "|      A007|    2000.0|  1|\n",
            "|      A007|     500.0|  2|\n",
            "+----------+----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qn 4.4. what are the second highest order values for each month?\n",
        "spark.sql(\"\"\"SELECT * FROM (SELECT ord_num, ord_date, ord_amount, DENSE_RANK() OVER(PARTITION BY DATE_PART('month', ord_date)\n",
        "        ORDER BY ord_amount DESC) order_rank\n",
        "    FROM orders\n",
        ") t\n",
        "WHERE order_rank = 2\n",
        "ORDER BY ord_date\n",
        "\"\"\").show()\n",
        "\n",
        "# spark.sql(\"\"\"SELECT month(ord_date), agent_code, ord_amount, CUME_DIST() OVER(PARTITION BY month(ord_date) ORDER BY ord_amount) t from orders\n",
        "# \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORCvwuRg3Lcd",
        "outputId": "245d3861-353a-4509-f2c9-b42264ff3b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+----------+\n",
            "|ord_num|  ord_date|ord_amount|order_rank|\n",
            "+-------+----------+----------+----------+\n",
            "| 200128|2008-01-07|    3500.0|         2|\n",
            "+-------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 write text to file and save as myfile.csv\n",
        "# Find the count of each letter in the file\n",
        "with open(\"/content/sample_data/myfile.csv\",\"w\") as f:\n",
        "  f.write(\"\"\"Hi Hello How are you Hi I am doing good\n",
        "Hi Ram how are you doing\n",
        "Hi Jack are you fine\n",
        "Hello all\n",
        "  \"\"\")\n",
        "df = spark.read.csv(\"/content/sample_data/myfile.csv\")\n",
        "#df.withColumn(\"char\",F.explode(F.split(F.lower(df._c0),\"\"))).drop(\"_c0\").groupBy(\"char\").count().show()\n",
        "df.createOrReplaceTempView(\"tab\")\n",
        "spark.sql(\"select t, count(1) from (select explode(split(_c0,'')) t from tab) a group by t  \").show()\n",
        "# spark.read.csv(\"sample_data/california_housing_test.csv\",header=True).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gxuxJOeJh3J",
        "outputId": "90838218-dd05-4a17-c0ac-14d873281681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "|  t|count(1)|\n",
            "+---+--------+\n",
            "|  l|       6|\n",
            "|  g|       3|\n",
            "|  m|       2|\n",
            "|  f|       1|\n",
            "|  n|       3|\n",
            "|  k|       1|\n",
            "|  e|       6|\n",
            "|  o|      11|\n",
            "|  h|       1|\n",
            "|  d|       3|\n",
            "|  J|       1|\n",
            "|  w|       2|\n",
            "|  y|       3|\n",
            "|  c|       1|\n",
            "|  u|       3|\n",
            "|  i|       7|\n",
            "|  R|       1|\n",
            "|  I|       1|\n",
            "|  a|       7|\n",
            "|  r|       3|\n",
            "+---+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VYvYE_j01A1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "spark.sql(\"\"\"\n",
        "with Tab as (select * from values\n",
        "(1,'y'),\n",
        "(1,'y'),\n",
        "(2,'n'),\n",
        "(2,'n'),\n",
        "(3,'y'),\n",
        "(3,'n'),\n",
        "(4,'y'),\n",
        "(4,'y') t(ID,FLAG))\n",
        "--expected id, YFlagCnt NFlagCnt\n",
        "select id\n",
        ", sum(case when flag='y' then 1 else 0 end) YFlagCnt\n",
        ", sum(case when flag='n' then 1 else 0 end) NFlagCnt\n",
        "from tab group by id\n",
        "\"\"\").show()\n",
        "\n",
        "# spark.sql('''\n",
        "# select id, coalesce(y,0) as yCnt, coalesce(n,0) as Ncnt from\n",
        "# (select id, flag from tab) t pivot (count(flag) for flag in ('y','n'))\n",
        "# ''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DpFupDfY_2d",
        "outputId": "c9e1e33e-9eb9-46e7-cc87-931772680a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+--------+\n",
            "| id|YFlagCnt|NFlagCnt|\n",
            "+---+--------+--------+\n",
            "|  1|       2|       0|\n",
            "|  2|       0|       2|\n",
            "|  3|       1|       1|\n",
            "|  4|       2|       0|\n",
            "+---+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''\n",
        "with tab as (select * from values\n",
        " (1,'2021-10-28','home')\n",
        ",(1,'2021-10-29','payment')\n",
        ",(1,'2021-10-30','management')\n",
        ",(1,'2021-10-28','home')\n",
        ",(1,'2021-10-28','home') t\n",
        "(userid, date, page_visited))\n",
        "\n",
        "select date\n",
        ", sum(case when page_visited='home' then 1 else 0 end) home_cnt\n",
        ", sum(case when page_visited='payment' then 1 else 0 end) payment_cnt\n",
        ", sum(case when page_visited='management' then 1 else 0 end) management_cnt\n",
        "from tab group by date\n",
        "''').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsRVwh6GY_QT",
        "outputId": "0af45f71-0be9-4632-aaa2-08238dd51e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+--------------+\n",
            "|      date|home_cnt|payment_cnt|management_cnt|\n",
            "+----------+--------+-----------+--------------+\n",
            "|2021-10-28|       3|          0|             0|\n",
            "|2021-10-29|       0|          1|             0|\n",
            "|2021-10-30|       0|          0|             1|\n",
            "+----------+--------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,3,4]\n",
        "b = a\n",
        "b[1] = 100\n",
        "print(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Ngfm5JY-3R",
        "outputId": "a33cee4f-7974-4f94-f181-cd68348c9674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 100, 4] [1, 100, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Column element in 2 list\n",
        "a = [1,2,3,4,5]\n",
        "b = [4,5,6,7,8]\n",
        "def find_common(a,b):\n",
        "  #return [x for x in a if x in b]\n",
        "  return list(set(a) & set(b))\n",
        "find_common(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg3VKfHKfj43",
        "outputId": "bebe4547-eaea-4676-a3d9-0d3bb342d1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 6, 7, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter([1,2,2,2, 4, 3, 8, 9, 6, 6])[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLUQLeCnXWJ",
        "outputId": "e8953d71-f56d-456c-cc59-718189b85e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "d = deque([3,4,5])\n",
        "d.append(6)\n",
        "d.appendleft(2)\n",
        "print(d)\n",
        "d.pop()\n",
        "d.popleft()\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G89v0cOpnXSx",
        "outputId": "9086873b-0e10-4992-b8b5-c027982642a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deque([2, 3, 4, 5, 6])\n",
            "deque([3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_XlKajEWnXO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ulpog5QnW3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utc9XM2aY91d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Work Count Program\n",
        "rdd = spark.sparkContext.textFile(\"/content/sample_data/myfile.csv\")\n",
        "lineRdd = rdd.flatMap(lambda line:line.split(\" \"))\n",
        "words = lineRdd.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)\n",
        "words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAk9plqaMkGy",
        "outputId": "04eeaf19-7f6d-4427-aa4c-13081ba2e7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('How', 1),\n",
              " ('are', 3),\n",
              " ('you', 3),\n",
              " ('I', 1),\n",
              " ('doing', 2),\n",
              " ('good', 1),\n",
              " ('how', 1),\n",
              " ('Jack', 1),\n",
              " ('', 3),\n",
              " ('Hi', 4),\n",
              " ('Hello', 2),\n",
              " ('am', 1),\n",
              " ('Ram', 1),\n",
              " ('fine', 1),\n",
              " ('all', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_xJOzfGpTueD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "select * from values 1,2,3,4,5 as t(a) anti join values (3,2),(4,5),(7,5),(8,0),(9,4) b(a,c) on t.a=b.a\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "Lu3yAjxuGuc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a9f22f-c653-47f6-8641-2bd482c10da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|  a|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  5|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qns 5 out = [\"ih\",\"rogir\"]\n",
        "lst = [\"hi\", \"rigor\"]\n",
        "\n",
        "lst = [ele[-1::-1] for ele in lst]\n",
        "print(lst)\n",
        "\n",
        "l = [1,1,1,2,3,3,4,4]\n",
        "\n",
        "[(x,l.count(x)) for x in set(l)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC6V5A9zUS7X",
        "outputId": "7342f793-78b5-4a25-b6c2-56267db45f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ih', 'rogir']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 3), (2, 1), (3, 2), (4, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "select * from\n",
        "values 1,1,2,2,null,2,4\n",
        "join\n",
        "values 1,2,2,3,3,null\n",
        "\"\"\").count()\n",
        "\n",
        "li = [1,[2,[3,[4]]]]\n",
        "# out = li = [1,2,3,4]\n",
        "flat_li = [item for sublist in li for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "flat_li\n",
        "\n",
        "nlist = []\n",
        "for item in li:\n",
        "  if isinstance(item,list):\n",
        "    nlist.extend(item)\n",
        "  else:\n",
        "    nlist.append(item)\n",
        "print(nlist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6XlWeNUbEvD",
        "outputId": "eeef7261-c05a-4212-9fa3-b68178a8cee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, [3, [4]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename multiple column in spark in one statement\n",
        "spark.sql(\"select null as a, null as b, null c\").toDF(\"x\",\"y\",\"z\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJgeaM_7a8c9",
        "outputId": "06c4e014-c190-4dc1-e019-3e55958da0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|   x|   y|   z|\n",
            "+----+----+----+\n",
            "|NULL|NULL|NULL|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "with mytemp_cte\n",
        "(\n",
        "  select * from values 10000,10000,3000,5000 T(sal)\n",
        "),\n",
        "mytemp1_cte\n",
        "(\n",
        "  select 1 as t\n",
        ")\n",
        "select sal, sal - avg(sal) over(partition by 1) diff from mytemp_cte join mytemp1_cte order by sal desc limit 10\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HboCRc5FCfat",
        "outputId": "c40eef04-b7bc-4344-f8de-6674a962ea5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|  sal|   diff|\n",
            "+-----+-------+\n",
            "|10000| 3000.0|\n",
            "|10000| 3000.0|\n",
            "| 5000|-2000.0|\n",
            "| 3000|-4000.0|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qn . 7 SQL  to fetch the firt name , emp_no  and the count of teh no of  times their salary change over the year whose employee no is 10001\n",
        "# Note :- The result should be grouped first_name and e.emp_no\n",
        "# Emp Table\n",
        "# Emp_no, first_name, gender\n",
        "# Salary table\n",
        "#  emp_No, salary , from_date, to_date\n",
        "# output:\n",
        "# First_nbame Emp_no, Count\n",
        "#  Sams 1001 2"
      ],
      "metadata": {
        "id": "pzQaywIHLYcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark.sql(\"\"\"\n",
        "# select * from values (10001,'Sam','M') t(emp_no,sal,gender)\n",
        "# \"\"\").show()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "with emptab (select * from values\n",
        "(10001,10000,to_date('2021-11-04'),to_date('2021-11-09')),\n",
        "(10001,12000,to_date('2022-11-09'),to_date('2022-03-09')),\n",
        "(10001,14000,to_date('2022-04-09'),to_date('2022-05-09')),\n",
        "(10001,16000,to_date('2022-05-04'),to_date('2022-07-09')),\n",
        "(10001,18000,to_date('2022-07-04'),to_date('2022-11-09'))\n",
        "t(emp_no,sal,from_date,to_date))\n",
        "\n",
        "select emp_no,year(from_date), count(*) from emptab where year(from_date) = year(current_date) - 1 group by emp_no, year(from_date)\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrJr9sLhOVr5",
        "outputId": "7ad115fc-5df0-4da9-a904-172bf0280998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+--------+\n",
            "|emp_no|year(from_date)|count(1)|\n",
            "+------+---------------+--------+\n",
            "| 10001|           2022|       4|\n",
            "+------+---------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "with student(\n",
        "  select * from values\n",
        "('a','math',44),\n",
        "('a','eng',54),\n",
        "('a','phy',74),\n",
        "('b','math',44),\n",
        "('b','eng',54),\n",
        "('b','phy',74)\n",
        "T(name,sub,marks))\n",
        "\n",
        "select * from (\n",
        "\n",
        "select name,sub,marks from student\n",
        "\n",
        ")\n",
        "pivot\n",
        "(\n",
        "  max(marks) for sub in ('math','phy','eng')\n",
        ")\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua4ZE9iwVe9f",
        "outputId": "e42fb572-e925-4a8c-a2a4-a8f53fa88fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+---+---+\n",
            "|name|math|phy|eng|\n",
            "+----+----+---+---+\n",
            "|   b|  44| 74| 54|\n",
            "|   a|  44| 74| 54|\n",
            "+----+----+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description : You are given a time series data, which is a clickstream of user activity. Perform Sessionization\n",
        "\n",
        "on the data {as per the session definition given below} and generate session ids.\n",
        "\n",
        "Expected Steps:\n",
        "1. Create in an input file with the data given below in any flat file format, preferably csv.\n",
        "2. Read the input data file into your program\n",
        "3. Use spark batch {PySpark/Spark-Scala} to add an additional column with name session_id and generate the session\n",
        "id's based on the following logic:\n",
        "    Session expires after inactivity of 30 minutes; because of inactivity, no clickstream will be generated.\n",
        "    Session remains active for a maximum duration of 2 hours (i.e., after every two hours a new session starts).\n",
        "4. Save the resultant data (original data, enriched with Session IDs) in a Parquet file format.   \n",
        "Given Data\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "  Timestamp             User_id\n",
        "  2021-05-01T11:00:00Z      u1\n",
        "  2021-05-01T13:13:00Z      u1\n",
        "  2021-05-01T15:00:00Z      u2\n",
        "  2021-05-01T11:25:00Z      u1\n",
        "  2021-05-01T15:15:00Z      u2\n",
        "  2021-05-01T02:13:00Z      u3\n",
        "  2021-05-03T02:15:00Z      u4\n",
        "  2021-05-02T11:45:00Z      u1\n",
        "  2021-05-02T11:00:00Z      u3\n",
        "  2021-05-03T12:15:00Z      u3\n",
        "  2021-05-03T11:00:00Z      u4\n",
        "  2021-05-03T21:00:00Z      u4\n",
        "  2021-05-04T19:00:00Z      u2\n",
        "  2021-05-04T09:00:00Z      u3\n",
        "  2021-05-04T08:15:00Z      u1\n",
        "```\n",
        "Expected Output\n",
        "\n",
        "  \n",
        "```\n",
        "TimeStamp             User_id  \n",
        "<b>Session_id(<userid>_<session_number>)</b>\n",
        "2021-05-01T11:00:00Z    u1     For example, u1_s1\n",
        "2021-05-01T13:13:00Z    u1     For example, u1_s2\n",
        "2021-05-01T15:00:00Z    u2     For example, u2_s1\n",
        "2021-05-01T11:25:00Z    u1     Please derive based on given logic\n",
        "2021-05-01T15:15:00Z    u2     Please derive based on given logic\n",
        "2021-05-01T02:13:00Z    u3     Please derive based on given logic\n",
        "2021-05-03T02:15:00Z    u4     Please derive based on given logic\n",
        "2021-05-02T11:45:00Z    u1     Please derive based on given logic\n",
        "2021-05-02T11:00:00Z    u3     Please derive based on given logic\n",
        "2021-05-03T12:15:00Z    u3     Please derive based on given logic\n",
        "2021-05-03T11:00:00Z    u4     Please derive based on given logic\n",
        "2021-05-03T21:00:00Z    u4     Please derive based on given logic\n",
        "2021-05-04T19:00:00Z    u2     Please derive based on given logic\n",
        "2021-05-04T09:00:00Z    u3     Please derive based on given logic\n",
        "2021-05-04T08:15:00Z    u1     Please derive based on given logic\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9zRRlg535x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m9TGfaEx_MCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n"
      ],
      "metadata": {
        "id": "EwK8KZI-_Znm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.sql('''\n",
        "select cast(Timestamp as timestamp) Timestamp,User_Id from values\n",
        "('2021-05-01T11:00:00Z','u1'),\n",
        "('2021-05-01T13:13:00Z','u1'),\n",
        "('2021-05-01T15:00:00Z','u2'),\n",
        "('2021-05-01T11:25:00Z','u1'),\n",
        "('2021-05-01T15:15:00Z','u2'),\n",
        "('2021-05-01T02:13:00Z','u3'),\n",
        "('2021-05-03T02:15:00Z','u4'),\n",
        "('2021-05-02T11:45:00Z','u1'),\n",
        "('2021-05-02T11:00:00Z','u3'),\n",
        "('2021-05-03T12:15:00Z','u3'),\n",
        "('2021-05-03T11:00:00Z','u4'),\n",
        "('2021-05-03T21:00:00Z','u4'),\n",
        "('2021-05-04T19:00:00Z','u2'),\n",
        "('2021-05-04T09:00:00Z','u3'),\n",
        "('2021-05-04T08:15:00Z','u1')\n",
        "t(Timestamp,User_Id)\n",
        "''').cache()\n"
      ],
      "metadata": {
        "id": "vzfPRBwEC6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "df1.orderBy(\"User_id\",\"Timestamp\")\\\n",
        ".withColumn(\"ltd\",F.expr(\"unix_timestamp(Timestamp) - unix_timestamp(lag(Timestamp) over(partition by user_id order by Timestamp))\")/60)\\\n",
        ".withColumn(\"session_id\",F.expr(\"sum(case when ltd > 30 then 1 else 0 end) over(partition by user_id order by Timestamp) + sum(case when ltd > 120 then 1 else 0 end) over(partition by user_id order by Timestamp)\"))\\\n",
        ".show(1000,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yvs4DdF6DvP",
        "outputId": "c513cd21-7729-4454-eafa-b3b5f364a66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+------+----------+\n",
            "|Timestamp          |User_Id|ltd   |session_id|\n",
            "+-------------------+-------+------+----------+\n",
            "|2021-05-01 11:00:00|u1     |NULL  |0         |\n",
            "|2021-05-01 11:25:00|u1     |25.0  |0         |\n",
            "|2021-05-01 13:13:00|u1     |108.0 |1         |\n",
            "|2021-05-02 11:45:00|u1     |1352.0|3         |\n",
            "|2021-05-04 08:15:00|u1     |2670.0|5         |\n",
            "|2021-05-01 15:00:00|u2     |NULL  |0         |\n",
            "|2021-05-01 15:15:00|u2     |15.0  |0         |\n",
            "|2021-05-04 19:00:00|u2     |4545.0|2         |\n",
            "|2021-05-01 02:13:00|u3     |NULL  |0         |\n",
            "|2021-05-02 11:00:00|u3     |1967.0|2         |\n",
            "|2021-05-03 12:15:00|u3     |1515.0|4         |\n",
            "|2021-05-04 09:00:00|u3     |1245.0|6         |\n",
            "|2021-05-03 02:15:00|u4     |NULL  |0         |\n",
            "|2021-05-03 11:00:00|u4     |525.0 |2         |\n",
            "|2021-05-03 21:00:00|u4     |600.0 |4         |\n",
            "+-------------------+-------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess_w = Window.partitionBy('User_id').orderBy(\"TimeStamp\")\n",
        "\n",
        "time_diff_minutes = (F.unix_timestamp(\"Timestamp\") - F.unix_timestamp(F.lag(\"Timestamp\").over(sess_w))) / 60"
      ],
      "metadata": {
        "id": "3j2vX2Kp57Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7k1w_cCt8MuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = Window.partitionBy(\"User_id\").orderBy(\"TimeStamp\")\n",
        "td = (F.unix_timestamp(F.col(\"timestamp\")) - F.unix_timestamp(F.lag(\"timestamp\").over(sw)) ) / 60"
      ],
      "metadata": {
        "id": "AzMd9pY8TQAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1D-ZtCQVoSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_boundary = (sum(when(time_diff_minutes >= 30, 1).otherwise(0)).over(session_window) +\n",
        "                    when(time_diff_minutes >= 120, 1).otherwise(0)).cast(\"string\")\n"
      ],
      "metadata": {
        "id": "kR7zZxDbVot3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scol = (sum(F.when(td >= 30,1).otherwise(0).over(sw) + F.when(td >= 120,1).otherwise(0) )).cast(StringType())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "yzR4s2DXVpM6",
        "outputId": "8b83e21f-8477-4da4-c0ab-737943c8c17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[NOT_ITERABLE] Column is not iterable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-7d787af691c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NOT_ITERABLE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"objectName\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         )\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.withColumn(\"session_id\", scol ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Y-fLSmH7S08s",
        "outputId": "58916221-d458-4bca-85b3-1bc523d5ed46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-5c36dab8f366>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"session_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscol\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from chatGPT\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, unix_timestamp, lag, when, sum\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sessionization\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "data = [(\"2021-05-01T11:00:00Z\", \"u1\"),\n",
        "        (\"2021-05-01T13:13:00Z\", \"u1\"),\n",
        "        (\"2021-05-01T15:00:00Z\", \"u2\"),\n",
        "        (\"2021-05-01T11:25:00Z\", \"u1\"),\n",
        "        (\"2021-05-01T15:15:00Z\", \"u2\"),\n",
        "        (\"2021-05-01T02:13:00Z\", \"u3\"),\n",
        "        (\"2021-05-03T02:15:00Z\", \"u4\"),\n",
        "        (\"2021-05-02T11:45:00Z\", \"u1\"),\n",
        "        (\"2021-05-02T11:00:00Z\", \"u3\"),\n",
        "        (\"2021-05-03T12:15:00Z\", \"u3\"),\n",
        "        (\"2021-05-03T11:00:00Z\", \"u4\"),\n",
        "        (\"2021-05-03T21:00:00Z\", \"u4\"),\n",
        "        (\"2021-05-04T19:00:00Z\", \"u2\"),\n",
        "        (\"2021-05-04T09:00:00Z\", \"u3\"),\n",
        "        (\"2021-05-04T08:15:00Z\", \"u1\")]\n",
        "\n",
        "columns = [\"timestamp\", \"user_id\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Convert timestamp to TimestampType\n",
        "df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "\n",
        "# Define Window\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
        "\n",
        "# Calculate time difference in seconds from the previous event\n",
        "df = df.withColumn(\"prev_timestamp\", lag(\"timestamp\").over(window_spec)) \\\n",
        "    .withColumn(\"time_diff\", (unix_timestamp(\"timestamp\") - unix_timestamp(\"prev_timestamp\")) / 60)\n",
        "\n",
        "# Determine session break (30 minutes or 2 hours max session duration)\n",
        "df = df.withColumn(\"new_session\", when((col(\"time_diff\") > 30) | (col(\"time_diff\").isNull()), 1).otherwise(0))\n",
        "\n",
        "# Assign session IDs\n",
        "df = df.withColumn(\"session_number\", sum(\"new_session\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))) \\\n",
        "    .withColumn(\"session_id\", col(\"user_id\") + \"_s\" + col(\"session_number\").cast(\"string\"))\n",
        "\n",
        "# Save result to Parquet\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-BgWzXg3nXL",
        "outputId": "f113ae5d-779d-4d6b-96ee-72146dca20c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+-------------------+---------+-----------+--------------+----------+\n",
            "|          timestamp|user_id|     prev_timestamp|time_diff|new_session|session_number|session_id|\n",
            "+-------------------+-------+-------------------+---------+-----------+--------------+----------+\n",
            "|2021-05-01 11:00:00|     u1|               NULL|     NULL|          1|             1|      NULL|\n",
            "|2021-05-01 11:25:00|     u1|2021-05-01 11:00:00|     25.0|          0|             1|      NULL|\n",
            "|2021-05-01 13:13:00|     u1|2021-05-01 11:25:00|    108.0|          1|             2|      NULL|\n",
            "|2021-05-02 11:45:00|     u1|2021-05-01 13:13:00|   1352.0|          1|             3|      NULL|\n",
            "|2021-05-04 08:15:00|     u1|2021-05-02 11:45:00|   2670.0|          1|             4|      NULL|\n",
            "|2021-05-01 15:00:00|     u2|               NULL|     NULL|          1|             1|      NULL|\n",
            "|2021-05-01 15:15:00|     u2|2021-05-01 15:00:00|     15.0|          0|             1|      NULL|\n",
            "|2021-05-04 19:00:00|     u2|2021-05-01 15:15:00|   4545.0|          1|             2|      NULL|\n",
            "|2021-05-01 02:13:00|     u3|               NULL|     NULL|          1|             1|      NULL|\n",
            "|2021-05-02 11:00:00|     u3|2021-05-01 02:13:00|   1967.0|          1|             2|      NULL|\n",
            "|2021-05-03 12:15:00|     u3|2021-05-02 11:00:00|   1515.0|          1|             3|      NULL|\n",
            "|2021-05-04 09:00:00|     u3|2021-05-03 12:15:00|   1245.0|          1|             4|      NULL|\n",
            "|2021-05-03 02:15:00|     u4|               NULL|     NULL|          1|             1|      NULL|\n",
            "|2021-05-03 11:00:00|     u4|2021-05-03 02:15:00|    525.0|          1|             2|      NULL|\n",
            "|2021-05-03 21:00:00|     u4|2021-05-03 11:00:00|    600.0|          1|             3|      NULL|\n",
            "+-------------------+-------+-------------------+---------+-----------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tab1\n",
        "tabA   tabB\n",
        "id     id\n",
        "1\t\t2\n",
        "2\t\t4\n",
        "3\t\tNull\n",
        "4\t\t2\n",
        "5\t\t4\n",
        "Null\tNull\n",
        "Select Count(*), Count(ColA), count(ColB) from Tab1;\n",
        "\n",
        "left join -> 5  (9)\n",
        "inner join -> 4 (6)\n",
        "right join -> 4 (6)\n",
        "full outer join -> 6"
      ],
      "metadata": {
        "id": "DQhz8vjeXNnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = spark.sql(\"\"\"\n",
        "select * from values\n",
        "(1),\n",
        "(2),\n",
        "(3),\n",
        "(4),\n",
        "(5),\n",
        "(Null) as T(id)\"\"\")\n",
        "d2 = spark.sql(\"\"\"select * from values\n",
        "(2),\n",
        "(4),\n",
        "(Null),\n",
        "(2),\n",
        "(4),\n",
        "(Null) as T(id)\"\"\")\n",
        "\n",
        "d1.join(d2,d1.id==d2.id,\"left\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cvuiyutAaAf",
        "outputId": "d65646a5-302d-48be-88c1-6ce0021d7e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|  id|  id|\n",
            "+----+----+\n",
            "|   1|NULL|\n",
            "|   2|   2|\n",
            "|   2|   2|\n",
            "|   3|NULL|\n",
            "|   4|   4|\n",
            "|   4|   4|\n",
            "|   5|NULL|\n",
            "|NULL|NULL|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tab1\n",
        "tabA   tabB\n",
        "id     id\n",
        "1\t\t   2\n",
        "2\t\t   4\n",
        "3\t\t   Null\n",
        "4\t\t   2\n",
        "5\t\t   4\n",
        "Null\t Null\n",
        "Select Count(*), Count(ColA), count(ColB) from Tab1;\n",
        "\n",
        "left:16 (8)\n",
        "right: 13 (6)\n",
        "inner: 16 (5)\n",
        "full:  16 (8)"
      ],
      "metadata": {
        "id": "aQcF7ej8-vI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CustomerTab, Order_Tab\n",
        "\n",
        "id, name     order_id, order_date"
      ],
      "metadata": {
        "id": "cdnSzBjfyr23"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VBiawAcyrV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Create a Spark session\n",
        "# spark = SparkSession.builder.appName(\"Sessionization\").getOrCreate()\n",
        "\n",
        "# Read the input CSV file\n",
        "# df = spark.read.csv(\"clickstream_data.csv\", header=True)\n",
        "\n",
        "# Convert the timestamp column to a timestamp type\n",
        "df = df.withColumn(\"Timestamp\", F.to_timestamp(df[\"Timestamp\"]))\n",
        "\n",
        "# Define sessionization logic\n",
        "session_window = Window.partitionBy(\"User_id\").orderBy(\"Timestamp\")\n",
        "\n",
        "# Calculate time difference in minutes from the previous row\n",
        "time_diff_minutes = (F.unix_timestamp(\"Timestamp\") - F.unix_timestamp(F.lag(\"Timestamp\").over(session_window))) / 60\n",
        "\n",
        "\n",
        "# Generate session IDs\n",
        "df = df.withColumn(\"Session_id\",\n",
        "F.concat_ws(\"_\", df[\"User_id\"],\n",
        "\n",
        "  (F.sum(F.when(time_diff_minutes >= 30, 1).otherwise(0)).over(session_window) + F.when(time_diff_minutes >= 120, 1).otherwise(0)).cast(StringType())))\n",
        "\n",
        "df.show()\n",
        "# # Save the result in Parquet format\n",
        "# df.write.parquet(\"sessionized_data.parquet\")\n",
        "\n",
        "# # Stop the Spark session\n",
        "# spark.stop()\n"
      ],
      "metadata": {
        "id": "Za-RTtfYoyWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df6b57-ab4d-4833-e3e2-b4fd139521ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+----------+\n",
            "|          Timestamp|User_Id|Session_id|\n",
            "+-------------------+-------+----------+\n",
            "|2021-05-01 11:00:00|     u1|      u1_0|\n",
            "|2021-05-01 11:25:00|     u1|      u1_0|\n",
            "|2021-05-01 13:13:00|     u1|      u1_1|\n",
            "|2021-05-02 11:45:00|     u1|      u1_3|\n",
            "|2021-05-04 08:15:00|     u1|      u1_4|\n",
            "|2021-05-01 15:00:00|     u2|      u2_0|\n",
            "|2021-05-01 15:15:00|     u2|      u2_0|\n",
            "|2021-05-04 19:00:00|     u2|      u2_2|\n",
            "|2021-05-01 02:13:00|     u3|      u3_0|\n",
            "|2021-05-02 11:00:00|     u3|      u3_2|\n",
            "|2021-05-03 12:15:00|     u3|      u3_3|\n",
            "|2021-05-04 09:00:00|     u3|      u3_4|\n",
            "|2021-05-03 02:15:00|     u4|      u4_0|\n",
            "|2021-05-03 11:00:00|     u4|      u4_2|\n",
            "|2021-05-03 21:00:00|     u4|      u4_3|\n",
            "+-------------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}